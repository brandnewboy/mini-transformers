import os
import requests
import math
import tiktoken
import torch
import torch.nn as nn
import torch.nn.functional as F


# Hyperparameters
batch_size = 4
context_length = 16
d_model = 64
num_blocks = 8
num_heads = 4
learning_rate = 1e-3
dropout = 0.1
max_iters = 500
eval_interval = 50
eval_iters = 20
device = "cuda" if torch.cuda.is_available() else "cpu"
TORCH_SEED = 1337
torch.manual_seed(TORCH_SEED)


# Load the training data
if not os.path.exists("D:/Coding_Personal/py/LLM_Learn_Transformers/data/sales_textbook.txt"):
    data_url = "https://huggingface.co/datasets/goendalf666/sales-textbook_for_convincing_and_selling/raw/main/sales_textbook.txt"
    with open("D:/Coding_Personal/py/LLM_Learn_Transformers/data/sales_textbook.txt", "wb") as f:
        f.write(requests.get(data_url).content)

with open("D:/Coding_Personal/py/LLM_Learn_Transformers/data/sales_textbook.txt", "r") as f:
        text = f.read()

# tokenize the text
enc = tiktoken.get_encoding('cl100k_base')
tokenized_text = enc.encode(text)
max_token_value = max(tokenized_text)

# split into train and validation sets
train_size = int(0.9 * len(tokenized_text))
train_data = torch.tensor(tokenized_text[:train_size], dtype=torch.long)
val_data = torch.tensor(tokenized_text[train_size:], dtype=torch.long)


'''
the feed forward network consists of two linear layers with a ReLU activation function in between.
'''
class FeedForwardNetWork(nn.Module):
    def __init__(self):
        super().__init__()
        self.linear1 = nn.Linear(d_model, d_model * 4)
        self.ReLU = nn.ReLU()
        self.linear2 = nn.Linear(d_model * 4, d_model)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x):
        x = self.linear1(x)
        x = self.ReLU(x)
        x = self.linear2(x)
        x = self.dropout(x)
        return x

'''
the scaled dot product attention is a type of attention mechanism that is used in the transformer model.
it use three inputs: Q, K, V.
Q is the query, K is the key, V is the value.
the attention mechanism is used to compute the attention weights between the query and the key.
the attention weights are then used to compute the attention output.
'''
class ScaledDotProductAttention(nn.Module):
    def __init__(self):
        super().__init__()
        self.q_lin = nn.Linear(d_model, d_model // num_heads, bias=False)
        self.k_lin = nn.Linear(d_model, d_model // num_heads, bias=False)
        self.v_lin = nn.Linear(d_model, d_model // num_heads, bias=False)
        self.register_buffer('mask', torch.tril(torch.ones(context_length, context_length)))

    def linear_attention_computation(self, xq, xk, xv):
        pass

    def attention_computation(self, xq, xk, xv, T):
        weights = xq @ xk.transpose(-2, -1) / (1.0 / math.sqrt(xk.size(-1)))
        weights = weights.masked_fill(self.mask[:T, :T] == 0, float('-inf'))
        weights = F.softmax(weights, dim=-1)
        out = weights @ xv
        return out

    def forward(self, x):
        B, T, C = x.shape  # Batch size, Time steps(current context_length), Channels(dimensions)
        Q = self.q_lin(x) # Q = Q + self.lora(Q)
        '''
        if this layer assigns a lora layer to it, the Q will be changed as follows:
        Q = self.q_lin(x) + self.lora(x) + r
        x * Wq + A*B*x + rq
        then, the self.lora(x) can be transfer to TEE
        xq + rq = self.lora(x) + rq
        the rq is a random noise, which is generated by the TEE
        the xq is the query, which is generated by the TEE
        the self.lora(x) is the lora layer, which is generated by the TEE
        '''
        K = self.k_lin(x)
        V = self.v_lin(x)

        # weights = Q @ K.transpose(-2, -1) / (1.0 / math.sqrt(K.size(-1)))
        # weights = weights.masked_fill(self.mask[:T, :T] == 0, float('-inf'))
        # weights = F.softmax(weights, dim=-1)
        # out = weights @ V

        # beta out = self.attention(Q, K, V) ?? self.linear_attention(Q, K, V)
        out = self.attention_computation(Q, K, V, T)
        return out
    '''
    TEE:
    re
    '''


'''
the multi head attention
'''
class MultiHeadAttention(nn.Module):
    def __init__(self):
        super().__init__()
        # beta = torch.tensor(???)
        # attention = beta
        # self.attention_heads = nn.ModuleList([ScaledDotProductAttention() for _ in range(num_heads)])
        self.attention_heads = nn.ModuleList([ScaledDotProductAttention() for _ in range(num_heads)])
        self.projection_layer = nn.Linear((d_model // num_heads) * num_heads, d_model)
        self.dropout = nn.Dropout(dropout)


    def forward(self, x):
        attention_outputs = [head(x) for head in self.attention_heads]
        attention_output = torch.cat(attention_outputs, dim=-1)
        attention_output = self.projection_layer(attention_output)
        attention_output = self.dropout(attention_output)
        return attention_output
'''
the transformer block
'''
class TransformerBlock(nn.Module):
    def __init__(self):
        super().__init__()
        self.attention = MultiHeadAttention()
        self.feed_forward = FeedForwardNetWork()
        self.layer_norm1 = nn.LayerNorm(d_model)
        self.layer_norm2 = nn.LayerNorm(d_model)

    def forward(self, x):
        attention_output = self.attention(x)
        x = x + attention_output
        x = self.layer_norm1(x)
        feed_forward_output = self.feed_forward(x)
        x = x + feed_forward_output
        x = self.layer_norm2(x)
        return x

class Model(nn.Module):
    def __init__(self):
        super().__init__()
        self.token_embedding_table = nn.Embedding(max_token_value + 1, d_model)
        self.transformer_blocks = nn.ModuleList([TransformerBlock() for _ in range(num_blocks)])
        self.final_linear_layer = nn.Linear(d_model, max_token_value + 1)

    def forward(self, idx, targets=None):
        B, T = idx.shape
        token_embeddings = self.token_embedding_table(idx)
        position_encoding_lookup_table = torch.zeros(context_length, d_model, device=device)  # initial with zeros with shape (context_length, d_model)
        position = torch.arange(0, context_length, dtype=torch.float).unsqueeze(1)
        # apply the sine & cosine
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))
        position_encoding_lookup_table[:, 0::2] = torch.sin(position * div_term)
        position_encoding_lookup_table[:, 1::2] = torch.cos(position * div_term)
        # position_encoding_lookup_table = position_encoding_lookup_table.unsqueeze(0).expand(batch_size, -1, -1)  # add batch to the first dimension
        position_embeddings = position_encoding_lookup_table[:T, :].to(device)
        x = token_embeddings + position_embeddings
        for block in self.transformer_blocks:
            x = block(x)

        logits = self.final_linear_layer(x)

        if targets is None:
            loss = None
        else:
            B, T, C = logits.shape
            logits = logits.view(B * T, C)
            targets = targets.view(B * T)
            # cross_entropy loss function will automatically apply softmax function to the logits
            loss = F.cross_entropy(logits, targets)
        return logits, loss

    def generate(self, idx, max_new_tokens=100):
        # idx is (B,T) array of indices in the current context
        for _ in range(max_new_tokens):
            # Crop idx to the max size of our positional embeddings table
            idx_crop = idx[:, -context_length:]
            # Get predictions
            logits, loss = self(idx_crop)
            # Get the last time step from logits where the dimensions of the logits are (B,T,C)
            logits_last_timestep = logits[:, -1, :]
            # Apply softmax to get probabilities
            probs = F.softmax(input=logits_last_timestep, dim=-1)
            # Sample from the probabilities' distribution.
            idx_next = torch.multinomial(input=probs, num_samples=1)
            # Append the sampled indexes idx_next to idx
            idx = torch.cat((idx, idx_next), dim=1)
        return idx


def get_batch(split: str):
    data = train_data if split == "train" else val_data
    idx = torch.randint(len(data) - context_length, (batch_size,))
    x = torch.stack([data[i:i + context_length] for i in idx])
    y = torch.stack([data[i + 1:i + context_length + 1] for i in idx])
    x, y = x.to(device), y.to(device)
    return x, y

@torch.no_grad()
def estimate_loss(model):
    out = {}
    model.eval()
    for split in ["train", "val"]:
        losses = torch.zeros(eval_iters)
        for k in range(eval_iters):
            X, Y = get_batch(split)
            logits, loss = model(X, Y)
            losses[k] = loss.item()
        out[split] = losses.mean()
    model.train()
    return out


def train(model):
    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)
    tracked_losses = list()
    for iter in range(max_iters):
        if iter % eval_interval == 0 or iter == max_iters - 1:
            losses = estimate_loss(model)
            tracked_losses.append(losses)
            print(f"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}")
        xb, yb = get_batch("train")

        xb, yb = get_batch('train')
        logits, loss = model(xb, yb)
        optimizer.zero_grad(set_to_none=True)
        loss.backward() # beta
        optimizer.step()

def save_model(model, type = 'all'):
    if not os.path.exists("data"):
        os.makedirs("data")
    if type == 'all':
        torch.save(model.state_dict(), "data/model.pth")
        return
    # Save token embedding weights
    torch.save(model.token_embedding_table.state_dict(), "data/token_embedding.pth")
    
    # Save transformer block weights
    for i, block in enumerate(model.transformer_blocks):
        torch.save(block.state_dict(), f"data/transformer_block_{i}.pth")
    
    # Save final linear layer weights
    torch.save(model.final_linear_layer.state_dict(), "data/final_linear.pth")


def load_model(model, type = 'all'):
    if type == 'all':
        model.load_state_dict(torch.load("data/model.pth"))
        return model
    # Load token embedding weights
    model.token_embedding_table.load_state_dict(torch.load("data/token_embedding.pth"))
    
    # Load transformer block weights
    for i, block in enumerate(model.transformer_blocks):
        block.load_state_dict(torch.load(f"data/transformer_block_{i}.pth"))
    
    # Load final linear layer weights
    model.final_linear_layer.load_state_dict(torch.load("data/final_linear.pth"))
    
    return model

# Evaluate the model
def evaluate_model(model):
    model.eval()
    start = 'The'
    start_ids = enc.encode(start)
    x = (torch.tensor(start_ids, dtype=torch.long, device=device)[None, ...])
    y = model.generate(x, max_new_tokens=100)
    print('---------------')
    print(enc.decode(y[0].tolist()))
    print('---------------')

def trainModelForExport(_model):
    _model.to(device)
    train(_model)

if __name__ == "__main__":
    # model = Model()
    # model.to(device)
    # train(model)
    #
    # # Save the model using the new strategy
    # save_model(model, 'all')
    
    # Create a new model instance and load weights
    new_model = Model()
    new_model.to(device)
    new_model = load_model(new_model, 'all')
    
    # Evaluate the loaded model
    evaluate_model(new_model)
    # evaluate_model(model)